------------------------------------------
	*** INITIALIZE THE JOB ***
------------------------------------------


1	Launching spark shell:

	spark-shell --master yarn \
	--deploy-mode client \
	--conf spark.ui.port=12322 \
	--num-executors 1 \
	--executor-memory 500M
	
2	view size of the dataset in hdfs:

	hadoop fs -du -s -h /public/retail_db/orders
	hadoop fs -du -s -h /public/retail_db/order_items
	
3	Changing Spark Configuration in run time in spark-shell
	
	sc.stop()
	import org.apache.spark.SparkContext
	import org.apache.spark.SparkConf
	val conf = new SparkConf().setMaster("yarn-client").setAppName("MyAppName")
	val sc = new SparkContext(conf)
	
------------------------------------------
	*** RDD ***
------------------------------------------
	
1	List --> RDD

	val list = (1 to 100).toList
	val listRdd = sc.parallelize(list)

2	Local File System --> RDD
	
	import scala.io.Source

	val productList = Source.fromFile("/root/aditya/data/retail_db/products/part-00000").getLines
	val productRdd = sc.parallelize(productList.toList)
	productRdd.take(10).foreach(println)
	
3	HDFS --> RDD

	val orderRdd = sc.textFile("/user/root/retail_db/orders/")
	orderRdd.first
	orderRdd.take(10).foreach(println)

	
------------------------------------------
	*** FILTERING THE DATA ***
------------------------------------------
	
1	Filtering only COMPLETED & CLOSED Orders 

	val orderFiltered = orderRdd.filter(x => {x.split(",")(3) == "CLOSED" || x.split(",")(3) == "COMPLETE"})
	orderFiltered.collect.foreach(println)
	orderFiltered.count

	no of orders				    = 68883 ($ orderRdd.count)
	no of COMPLETED & CLOSED orders = 30455 ($ orderFiltered.count)
	
------------------------------------------
	*** ACCUMULATORS ***
------------------------------------------


